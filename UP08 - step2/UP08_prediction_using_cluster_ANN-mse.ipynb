{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='-1'\n",
    "os.environ['TF_CUDNN_USE_AUTOTUNE'] ='0'\n",
    "\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "\n",
    "rn.seed(14)\n",
    "np.random.seed(14)\n",
    "tf.random.set_seed(14)\n",
    "\n",
    "# from keras import backend as k\n",
    "import tensorflow.keras.backend as k\n",
    "\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1,\n",
    "allow_soft_placement=True, device_count = {'CPU': 1})\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(),config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from math import sqrt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "import keras_tuner as kt\n",
    "\n",
    "import seaborn as sn\n",
    "from matplotlib import style\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "from keras.layers import Dense, Dropout,RepeatVector, Input,Flatten, Conv1D,MaxPooling1D\n",
    "rn.seed(14)\n",
    "np.random.seed(14)\n",
    "tf.random.set_seed(14)\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Datasets:\n",
    "# Cluster 1: November, December, January, February\n",
    "# Cluster 2: June, July, August\n",
    "# Cluster 3: April, May, September, October, March"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 1: November, December, January, February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your original dataset\n",
    "df = pd.read_excel(r\"UP08.xlsx\", index_col='Date_Time')\n",
    "df = df[['dissolvedoxygenmeasured']]\n",
    "df=df['2017-04-01':'2021-03-31']\n",
    "\n",
    "# Filter DataFrame for the months November, December, January, February\n",
    "df = df[df.index.month.isin([11, 12, 1, 2])]\n",
    "\n",
    "# Print or use the resulting DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Assuming you have a time series data stored in a pandas DataFrame called 'data'\n",
    "# with a column named 'value' containing the values\n",
    "\n",
    "# Convert the 'value' column to a numpy array\n",
    "# values = data['value'].to_numpy()\n",
    "\n",
    "# Plot ACF\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "plot_acf(df['dissolvedoxygenmeasured'][:100], ax=ax,alpha=0.05)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "plot_pacf(df['dissolvedoxygenmeasured'][:100], ax=ax,alpha=0.05)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Partial Autocorrelation')\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df['2017-04-01':'2020-03-31']\n",
    "df_test=df['2020-04-01':]\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "import numpy \n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back+1, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def percentage_error(actual, predicted):\n",
    "    res = numpy.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return numpy.mean(numpy.abs(percentage_error(numpy.asarray(y_true), numpy.asarray(y_pred)))) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=df.values\n",
    "X, Y = create_dataset(datasets, 9)\n",
    "\n",
    "train_size = df_train.shape[0]\n",
    "test_size =  df_test.shape[0]\n",
    "trainX, testX = X[0:train_size], X[train_size:]\n",
    "trainY, testY = Y[0:train_size], Y[train_size:]\n",
    "X_train=pd.DataFrame(trainX)\n",
    "Y_train=pd.DataFrame(trainY)\n",
    "X_test=pd.DataFrame(testX)\n",
    "Y_test=pd.DataFrame(testY)\n",
    "sc_X = MinMaxScaler()\n",
    "sc_y = MinMaxScaler()\n",
    "X= sc_X.fit_transform(X_train)\n",
    "y= sc_y.fit_transform(Y_train)\n",
    "X1= sc_X.transform(X_test)\n",
    "y1= sc_y.transform(Y_test)\n",
    "y=y.ravel()\n",
    "y1=y1.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rn.seed(14)\n",
    "np.random.seed(14)\n",
    "tf.random.set_seed(14)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 32, activation='relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(8))\n",
    "model.add(Dense(4))\n",
    "# model.add(Dense(3))\n",
    "model.add(Dense(2))\n",
    "model.add(Dense(1))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='mse',optimizer=opt)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,verbose=1,mode=\"min\")\n",
    "mcp_save = ModelCheckpoint('.UP08_c1_mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "model.fit(trainX, y, validation_split = 0.1, epochs = 1000, batch_size = 64,verbose=1\n",
    "          , callbacks =[es, mcp_save, reduce_lr_loss]\n",
    "         ) \n",
    "\n",
    "\n",
    "y_pred_train = model.predict(trainX)\n",
    "y_pred_test = model.predict(testX)\n",
    "\n",
    "y_pred_test= np.array(y_pred_test).ravel()\n",
    "y_pred_test=pd.DataFrame(y_pred_test)\n",
    "\n",
    "y1=pd.DataFrame(y1)\n",
    "y=pd.DataFrame(y)                           \n",
    "\n",
    "y_pred_train= np.array(y_pred_train).ravel()\n",
    "y_pred_train=pd.DataFrame(y_pred_train)\n",
    "\n",
    "y_test= sc_y.inverse_transform (y1)\n",
    "y_train= sc_y.inverse_transform (y)\n",
    "y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
    "y_pred_train1= sc_y.inverse_transform (y_pred_train)      \n",
    "\n",
    "mape_train1_c1= mean_absolute_percentage_error(y_train,y_pred_train1)\n",
    "rmse_train1_c1= sqrt(mean_squared_error(y_train,y_pred_train1))\n",
    "mae_train1_c1= mean_absolute_error(y_train,y_pred_train1)\n",
    "\n",
    "mape_test1_c1= mean_absolute_percentage_error(y_test,y_pred_test1)\n",
    "rmse_test1_c1= sqrt(mean_squared_error(y_test,y_pred_test1))\n",
    "mae_test1_c1= mean_absolute_error(y_test,y_pred_test1)\n",
    "\n",
    "\n",
    "print(\" \\n train results\")\n",
    "print(mape_train1_c1)\n",
    "print(rmse_train1_c1)\n",
    "print(mae_train1_c1)\n",
    "\n",
    "print(\"test results\")\n",
    "print(mape_test1_c1)\n",
    "print(rmse_test1_c1)\n",
    "print(mae_test1_c1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 2: June, July, August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your original dataset\n",
    "df = pd.read_excel(r\"UP08.xlsx\", index_col='Date_Time')\n",
    "df = df[['dissolvedoxygenmeasured']]\n",
    "df=df['2017-04-01':'2021-03-31']\n",
    "\n",
    "# Assuming you have already loaded and filtered your DataFrame df\n",
    "# If not, you can use the provided code to load and filter it\n",
    "\n",
    "# Filter DataFrame for the months 'June', 'July', 'August'\n",
    "df = df[df.index.month.isin([6, 7, 8])]\n",
    "\n",
    "# Print or use the resulting DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Assuming you have a time series data stored in a pandas DataFrame called 'data'\n",
    "# with a column named 'value' containing the values\n",
    "\n",
    "# Convert the 'value' column to a numpy array\n",
    "# values = data['value'].to_numpy()\n",
    "\n",
    "# Plot ACF\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "plot_acf(df['dissolvedoxygenmeasured'][:100], ax=ax,alpha=0.05)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "plot_pacf(df['dissolvedoxygenmeasured'][:100], ax=ax,alpha=0.05)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Partial Autocorrelation')\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train=df['2017-04-01':'2020-03-31']\n",
    "df_test=df['2020-04-01':]\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "import numpy \n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back+1, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def percentage_error(actual, predicted):\n",
    "    res = numpy.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return numpy.mean(numpy.abs(percentage_error(numpy.asarray(y_true), numpy.asarray(y_pred)))) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=df.values\n",
    "X, Y = create_dataset(datasets, 5)\n",
    "\n",
    "train_size = df_train.shape[0]\n",
    "test_size =  df_test.shape[0]\n",
    "trainX, testX = X[0:train_size], X[train_size:]\n",
    "trainY, testY = Y[0:train_size], Y[train_size:]\n",
    "X_train=pd.DataFrame(trainX)\n",
    "Y_train=pd.DataFrame(trainY)\n",
    "X_test=pd.DataFrame(testX)\n",
    "Y_test=pd.DataFrame(testY)\n",
    "sc_X = MinMaxScaler()\n",
    "sc_y = MinMaxScaler()\n",
    "X= sc_X.fit_transform(X_train)\n",
    "y= sc_y.fit_transform(Y_train)\n",
    "X1= sc_X.transform(X_test)\n",
    "y1= sc_y.transform(Y_test)\n",
    "y=y.ravel()\n",
    "y1=y1.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rn.seed(14)\n",
    "np.random.seed(14)\n",
    "tf.random.set_seed(14)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 8, activation='relu'))\n",
    "# model.add(Dense(16))\n",
    "model.add(Dense(8))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(2))\n",
    "model.add(Dense(1))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='mse',optimizer=opt)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,verbose=1,mode=\"min\")\n",
    "mcp_save = ModelCheckpoint('.UP08_c2_mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "model.fit(trainX, y, validation_split = 0.1, epochs = 1000, batch_size = 64,verbose=1\n",
    "          , callbacks =[es, mcp_save, reduce_lr_loss]\n",
    "         ) \n",
    "\n",
    "y_pred_train = model.predict(trainX)\n",
    "y_pred_test = model.predict(testX)\n",
    "\n",
    "y_pred_test= np.array(y_pred_test).ravel()\n",
    "y_pred_test=pd.DataFrame(y_pred_test)\n",
    "\n",
    "y1=pd.DataFrame(y1)\n",
    "y=pd.DataFrame(y)                           \n",
    "\n",
    "y_pred_train= np.array(y_pred_train).ravel()\n",
    "y_pred_train=pd.DataFrame(y_pred_train)\n",
    "\n",
    "y_test= sc_y.inverse_transform (y1)\n",
    "y_train= sc_y.inverse_transform (y)\n",
    "y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
    "y_pred_train1= sc_y.inverse_transform (y_pred_train)      \n",
    "\n",
    "mape_train1_c2= mean_absolute_percentage_error(y_train,y_pred_train1)\n",
    "rmse_train1_c2= sqrt(mean_squared_error(y_train,y_pred_train1))\n",
    "mae_train1_c2= mean_absolute_error(y_train,y_pred_train1)\n",
    "\n",
    "mape_test1_c2= mean_absolute_percentage_error(y_test,y_pred_test1)\n",
    "rmse_test1_c2= sqrt(mean_squared_error(y_test,y_pred_test1))\n",
    "mae_test1_c2= mean_absolute_error(y_test,y_pred_test1)\n",
    "\n",
    "\n",
    "print(\" \\n train results\")\n",
    "print(mape_train1_c2)\n",
    "print(rmse_train1_c2)\n",
    "print(mae_train1_c2)\n",
    "\n",
    "print(\"test results\")\n",
    "print(mape_test1_c2)\n",
    "print(rmse_test1_c2)\n",
    "print(mae_test1_c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 3: April, May, September, October, March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load your original dataset\n",
    "df = pd.read_excel(r\"UP08.xlsx\", index_col='Date_Time')\n",
    "df = df[['dissolvedoxygenmeasured']]\n",
    "df=df['2017-04-01':'2021-03-31']\n",
    "\n",
    "\n",
    "# Filter DataFrame for the months April, May, September, October, March\n",
    "\n",
    "df = df[df.index.month.isin([3, 4, 5, 9, 10])]\n",
    "\n",
    "# Print or use the resulting DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Assuming you have a time series data stored in a pandas DataFrame called 'data'\n",
    "# with a column named 'value' containing the values\n",
    "\n",
    "# Convert the 'value' column to a numpy array\n",
    "# values = data['value'].to_numpy()\n",
    "\n",
    "# Plot ACF\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "plot_acf(df['dissolvedoxygenmeasured'][:100], ax=ax,alpha=0.05)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "plot_pacf(df['dissolvedoxygenmeasured'][:100], ax=ax,alpha=0.05)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Partial Autocorrelation')\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df['2017-04-01':'2020-03-31']\n",
    "df_test=df['2020-04-01':]\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "import numpy \n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back+1, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def percentage_error(actual, predicted):\n",
    "    res = numpy.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return numpy.mean(numpy.abs(percentage_error(numpy.asarray(y_true), numpy.asarray(y_pred)))) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=df.values\n",
    "X, Y = create_dataset(datasets, 8)\n",
    "\n",
    "train_size = df_train.shape[0]\n",
    "test_size =  df_test.shape[0]\n",
    "trainX, testX = X[0:train_size], X[train_size:]\n",
    "trainY, testY = Y[0:train_size], Y[train_size:]\n",
    "X_train=pd.DataFrame(trainX)\n",
    "Y_train=pd.DataFrame(trainY)\n",
    "X_test=pd.DataFrame(testX)\n",
    "Y_test=pd.DataFrame(testY)\n",
    "sc_X = MinMaxScaler()\n",
    "sc_y = MinMaxScaler()\n",
    "X= sc_X.fit_transform(X_train)\n",
    "y= sc_y.fit_transform(Y_train)\n",
    "X1= sc_X.transform(X_test)\n",
    "y1= sc_y.transform(Y_test)\n",
    "y=y.ravel()\n",
    "y1=y1.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rn.seed(14)\n",
    "np.random.seed(14)\n",
    "tf.random.set_seed(14)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 32, activation='relu'))\n",
    "model.add(Dense(units = 16))\n",
    "model.add(Dense(8))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(2))\n",
    "model.add(Dense(1))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='mse',optimizer=opt)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,verbose=1,mode=\"min\")\n",
    "mcp_save = ModelCheckpoint('.UP08_c3_mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "model.fit(trainX, y, validation_split = 0.1, epochs = 1000, batch_size = 64,verbose=1\n",
    "          , callbacks =[es, mcp_save, reduce_lr_loss]\n",
    "         ) \n",
    "\n",
    "\n",
    "y_pred_train = model.predict(trainX)\n",
    "y_pred_test = model.predict(testX)\n",
    "\n",
    "y_pred_test= np.array(y_pred_test).ravel()\n",
    "y_pred_test=pd.DataFrame(y_pred_test)\n",
    "\n",
    "y1=pd.DataFrame(y1)\n",
    "y=pd.DataFrame(y)                           \n",
    "\n",
    "y_pred_train= np.array(y_pred_train).ravel()\n",
    "y_pred_train=pd.DataFrame(y_pred_train)\n",
    "\n",
    "y_test= sc_y.inverse_transform (y1)\n",
    "y_train= sc_y.inverse_transform (y)\n",
    "y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
    "y_pred_train1= sc_y.inverse_transform (y_pred_train)      \n",
    "\n",
    "mape_train1_c3= mean_absolute_percentage_error(y_train,y_pred_train1)\n",
    "rmse_train1_c3= sqrt(mean_squared_error(y_train,y_pred_train1))\n",
    "mae_train1_c3= mean_absolute_error(y_train,y_pred_train1)\n",
    "\n",
    "mape_test1_c3= mean_absolute_percentage_error(y_test,y_pred_test1)\n",
    "rmse_test1_c3= sqrt(mean_squared_error(y_test,y_pred_test1))\n",
    "mae_test1_c3= mean_absolute_error(y_test,y_pred_test1)\n",
    "\n",
    "\n",
    "print(\" \\n train results\")\n",
    "print(mape_train1_c3)\n",
    "print(rmse_train1_c3)\n",
    "print(mae_train1_c3)\n",
    "\n",
    "print(\"test results\")\n",
    "print(mape_test1_c3)\n",
    "print(rmse_test1_c3)\n",
    "print(mae_test1_c3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your original dataset\n",
    "df = pd.read_excel(r\"UP08.xlsx\", index_col='Date_Time')\n",
    "df = df[['dissolvedoxygenmeasured']]\n",
    "df=df['2017-04-01':'2021-03-31']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Assuming you have a time series data stored in a pandas DataFrame called 'data'\n",
    "# with a column named 'value' containing the values\n",
    "\n",
    "# Convert the 'value' column to a numpy array\n",
    "# values = data['value'].to_numpy()\n",
    "\n",
    "# Plot ACF\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "plot_acf(df['dissolvedoxygenmeasured'][:100], ax=ax,alpha=0.05)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "plot_pacf(df['dissolvedoxygenmeasured'][:100], ax=ax,alpha=0.05)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Partial Autocorrelation')\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "import numpy \n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back+1, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def percentage_error(actual, predicted):\n",
    "    res = numpy.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return numpy.mean(numpy.abs(percentage_error(numpy.asarray(y_true), numpy.asarray(y_pred)))) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data = df.loc['2017-04-01':'2020-03-31']\n",
    "test_data = df.loc['2020-04-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=df.values\n",
    "X, Y = create_dataset(datasets, 4)\n",
    "\n",
    "train_size = train_data.shape[0]\n",
    "test_size =  test_data.shape[0]\n",
    "trainX, testX = X[0:train_size], X[train_size:]\n",
    "trainY, testY = Y[0:train_size], Y[train_size:]\n",
    "X_train=pd.DataFrame(trainX)\n",
    "Y_train=pd.DataFrame(trainY)\n",
    "X_test=pd.DataFrame(testX)\n",
    "Y_test=pd.DataFrame(testY)\n",
    "sc_X = MinMaxScaler()\n",
    "sc_y = MinMaxScaler()\n",
    "X= sc_X.fit_transform(X_train)\n",
    "y= sc_y.fit_transform(Y_train)\n",
    "X1= sc_X.transform(X_test)\n",
    "y1= sc_y.transform(Y_test)\n",
    "y=y.ravel()\n",
    "y1=y1.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rn.seed(14)\n",
    "np.random.seed(14)\n",
    "tf.random.set_seed(14)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 16, activation='relu'))\n",
    "model.add(Dense(units = 8))\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(2))\n",
    "model.add(Dense(1))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='mse',optimizer=opt)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=5,verbose=1,mode=\"min\")\n",
    "mcp_save = ModelCheckpoint('.UP08_mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "model.fit(trainX, y, validation_split = 0.1, epochs = 1000, batch_size = 64,verbose=1\n",
    "          , callbacks =[es, mcp_save, reduce_lr_loss]\n",
    "         ) \n",
    "\n",
    "y_pred_train = model.predict(trainX)\n",
    "y_pred_test = model.predict(testX)\n",
    "\n",
    "y_pred_test= np.array(y_pred_test).ravel()\n",
    "y_pred_test=pd.DataFrame(y_pred_test)\n",
    "\n",
    "y1=pd.DataFrame(y1)\n",
    "y=pd.DataFrame(y)                           \n",
    "\n",
    "y_pred_train= np.array(y_pred_train).ravel()\n",
    "y_pred_train=pd.DataFrame(y_pred_train)\n",
    "\n",
    "y_test= sc_y.inverse_transform (y1)\n",
    "y_train= sc_y.inverse_transform (y)\n",
    "y_pred_test1= sc_y.inverse_transform (y_pred_test)\n",
    "y_pred_train1= sc_y.inverse_transform (y_pred_train)      \n",
    "\n",
    "mape_train1= mean_absolute_percentage_error(y_train,y_pred_train1)\n",
    "rmse_train1= sqrt(mean_squared_error(y_train,y_pred_train1))\n",
    "mae_train1= mean_absolute_error(y_train,y_pred_train1)\n",
    "\n",
    "mape_test1= mean_absolute_percentage_error(y_test,y_pred_test1)\n",
    "rmse_test1= sqrt(mean_squared_error(y_test,y_pred_test1))\n",
    "mae_test1= mean_absolute_error(y_test,y_pred_test1)\n",
    "\n",
    "\n",
    "print(\" \\n train results\")\n",
    "print(mape_train1)\n",
    "print(rmse_train1)\n",
    "print(mae_train1)\n",
    "\n",
    "print(\"test results\")\n",
    "print(mape_test1)\n",
    "print(rmse_test1)\n",
    "print(mae_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using whole dataset vs clusters for prediction \n",
    "print(mape_test1, mape_test1_c1,mape_test1_c2,mape_test1_c3)\n",
    "print(rmse_test1, rmse_test1_c1,rmse_test1_c2,rmse_test1_c3)\n",
    "print(mae_test1, mae_test1_c1,mae_test1_c2,mae_test1_c3)\n",
    "\n",
    "print(\"\\n\")\n",
    "# using whole dataset vs clusters for prediction \n",
    "print(mape_test1, (((4/12)*mape_test1_c1)+((3/12)*mape_test1_c2)+((5/12)*mape_test1_c3)))\n",
    "print(rmse_test1, (((4/12)*rmse_test1_c1)+((3/12)*rmse_test1_c2)+((5/12)*rmse_test1_c3)))\n",
    "print(mae_test1, (((4/12)*mae_test1_c1)+((3/12)*mae_test1_c2)+((5/12)*mae_test1_c3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
